{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a0646d-dccc-4ac5-bd74-8bf079f5a6a1",
   "metadata": {},
   "source": [
    "# CountryGuessr - Country Prediction from Street View Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33cb7ec9-4f0b-4d4e-97bd-325e9aeabc76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "%pip install -q torch torchvision\n",
    "%pip install -q pycountry tqdm matplotlib seaborn tensorboard kagglehub timm evaluate grad-cam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4879e94f-f17f-40b9-884f-9e356095ab04",
   "metadata": {},
   "source": [
    "# Download Dataset\n",
    "Use example dataset from Kaggle for initial setup, since we do not have a fixed dataset yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc03cf1f-8ab2-4382-93ec-3451cf801d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/CountryGuessr/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/jovyan/.cache/kagglehub/datasets/sylshaw/streetview-by-country/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sylshaw/streetview-by-country\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab0acd-58d9-49e4-be9a-70f2830c464f",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fea3b34-7c3c-4fb2-abca-a7b5384c5685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Seed set to: 42\n",
      "Num epochs: 5, Batch size: 32, Learning rate: 0.0001\n",
      "TensorBoard log dir: runs/country_classifier\n",
      "Model checkpoint dir: model\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set random seeds\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 1e-4\n",
    "LR_SCHEDULER_PATIENCE = 2\n",
    "LR_SCHEDULER_FACTOR = 0.1\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "LOG_DIR = \"runs/country_classifier\"\n",
    "CHECKPOINT_DIR = \"model\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Seed set to: {RANDOM_SEED}\")\n",
    "print(f\"Num epochs: {NUM_EPOCHS}, Batch size: {BATCH_SIZE}, Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"TensorBoard log dir: {LOG_DIR}\")\n",
    "print(f\"Model checkpoint dir: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc107d",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "- Split into training, validation, test sets\n",
    "- Put data in correct format -> each country represents a separate class\n",
    "- Create label map to map country code to country name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ddd2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from src.dataset import StreetViewDataset\n",
    "\n",
    "\n",
    "DATASET_DIR = os.path.join(path, \"streetview_images\")\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.Resize((224, 224)),\n",
    "  transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = StreetViewDataset(DATASET_DIR, transform=transform)\n",
    "train_size = int(TRAIN_SPLIT * len(dataset))\n",
    "val_size = int(VAL_SPLIT * len(dataset))\n",
    "test_size = len(dataset) - (train_size + val_size)\n",
    "\n",
    "train_set, val_set, test_set = random_split(\n",
    "  dataset,\n",
    "  [train_size, val_size, test_size],\n",
    "  generator=torch.Generator().manual_seed(RANDOM_SEED)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_set, BATCH_SIZE)\n",
    "test_loader = DataLoader(test_set, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3006d",
   "metadata": {},
   "source": [
    "# Model Choices\n",
    "There are 3 different model options used here to compare performance between model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f72c64-1ef9-43f1-831a-31ec0efb9b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "num_classes = len(dataset.label_map)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d921ccdd-6214-4853-96ac-9c6c9d9b025b",
   "metadata": {},
   "source": [
    "### ResNet50 (CNN) - [Docs](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html)\n",
    "- CNN architecture with added residual (skip) connections\n",
    "- Enables deeper networks without training difficulties (Vanishing Gradient, Degradation Problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8b6686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/CountryGuessr/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jovyan/CountryGuessr/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from src.model import ResNet50Model\n",
    "\n",
    "model = ResNet50Model(num_classes, pretrained=True)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f29de5-5e46-43ad-b9a1-655279740b21",
   "metadata": {},
   "source": [
    "### Base Vision Transformer (ViT) - [Docs](https://huggingface.co/docs/transformers/model_doc/vit)\n",
    "- Transformer for computer vision tasks\n",
    "- Splits images into fixed-sized patches, treating them as a sequence of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "985d85d9-c141-4956-bf2a-558674c8e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import ViTModel\n",
    "\n",
    "model = ViTModel(num_classes, pretrained=True)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5347c2b-bfb0-43b0-9132-30345b752cd0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Swin Transformer -  [Docs](https://huggingface.co/docs/transformers/model_doc/swin)\n",
    "\n",
    "Could be good model for future - **not working yet**\n",
    "\n",
    "- Hierarchical vision transformer with shifted windows (swin)\n",
    "- Divides images into patches and applies windowed self-attention to capture local features\n",
    "- Uses shifted windows to enable cross-window connections and global context modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73acfa84-3648-46b9-a3fd-4d5171ef8a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import SwinTransformerModel\n",
    "\n",
    "num_classes = len(dataset.label_map)\n",
    "\n",
    "model = SwinTransformerModel(num_classes, pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c07c0b",
   "metadata": {},
   "source": [
    "# Training/Validation\n",
    "\n",
    "## Logging\n",
    "- Log training loss and accuracy during training phase\n",
    "- Compute and log validation loss, accuracy, precision, recall, and F1-Score\n",
    "- Generate and log confusion matrix heatmap images\n",
    "- Visualize all metrics and images using TensorBoard\n",
    "\n",
    "## Learning Rate Scheduler\n",
    "- \"ReduceLROnPlateau\" scheduler to reduce learning rate when validation loss plateaus\n",
    "- Helps to fine-tune training progression and prevent getting stuck in local minima\n",
    "\n",
    "## Early Stopping\n",
    "- Monitor validation loss to detect noo improvement over specified patience period\n",
    "- Stops training early if no improvement is observed\n",
    "\n",
    "## Checkpoint Saving\n",
    "- Save model checkpoints at the end of each epoch\n",
    "- Update a dedicated \"best\" checkpoint whenever validation loss improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ViTModel\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/CountryGuessr/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to model/ViTModel/epoch_0.pth\n",
      "Epoch 1/5 - train_loss=3.1060, train_acc=0.2465\n",
      "Checkpoint saved to model/ViTModel/best.pth\n",
      "Best model updated at Epoch 1 with val_loss=2.1011\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|‚ñè         | 44/2682 [00:09<09:52,  4.45it/s, acc=0.4929, loss=1.8391]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src.train import train_epoch, validate_epoch, save_checkpoint\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=LR_SCHEDULER_FACTOR, patience=LR_SCHEDULER_PATIENCE)\n",
    "writer = SummaryWriter(\"runs/country_classifier\")\n",
    "\n",
    "# Load metrics\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "confusion_metric = evaluate.load(\"confusion_matrix\")\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=False, cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    buf.seek(0)\n",
    "    plt.close()\n",
    "\n",
    "    image = plt.imread(buf)\n",
    "    buf.close()\n",
    "    return image\n",
    "\n",
    "global_step = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "print(f\"Using {model.__class__.__name__}\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "    # Train/Val\n",
    "    train_loss, train_acc, global_step = train_epoch(model, train_loader, criterion, optimizer, DEVICE, writer, global_step)\n",
    "    val_loss, val_acc, val_preds, val_labels, global_step = validate_epoch(model, val_loader, criterion, DEVICE, writer, global_step)\n",
    "\n",
    "    # Quantitative Metrics\n",
    "    precision_metric.add_batch(predictions=val_preds, references=val_labels)\n",
    "    val_precision = precision_metric.compute(average=\"macro\")[\"precision\"]\n",
    "\n",
    "    recall_metric.add_batch(predictions=val_preds, references=val_labels)\n",
    "    val_recall = recall_metric.compute(average=\"macro\")[\"recall\"]\n",
    "    \n",
    "    f1_metric.add_batch(predictions=val_preds, references=val_labels)\n",
    "    val_f1 = f1_metric.compute(average=\"macro\")[\"f1\"]\n",
    "\n",
    "    confusion_metric.add_batch(predictions=val_preds, references=val_labels)\n",
    "    cm_result = confusion_metric.compute()[\"confusion_matrix\"]\n",
    "    cm_image = plot_confusion_matrix(cm_result, dataset.label_map)\n",
    "    cm_tensor = transforms.ToTensor()(cm_image).unsqueeze(0)\n",
    "\n",
    "    # Tensorboard logging\n",
    "    writer.add_scalar(\"Validation/Precision\", val_precision, epoch)\n",
    "    writer.add_scalar(\"Validation/Recall\", val_recall, epoch)\n",
    "    writer.add_scalar(\"Validation/F1-Score\", val_f1, epoch)\n",
    "    writer.add_image(\"Validation/Confusion_Matrix\", cm_tensor[0], epoch)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Store checkpoint after epoch\n",
    "    save_checkpoint(model, optimizer, epoch, dataset.label_map, train_loss=train_loss, val_loss=val_loss, train_acc=train_acc, val_acc=val_acc, checkpoint_dir=os.path.join(CHECKPOINT_DIR, model.__class__.__name__))\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - train_loss={train_loss:.4f}, train_acc={train_acc:.4f}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "        # Update best checkpoint if val_loss decreases\n",
    "        save_checkpoint(model, optimizer, epoch, dataset.label_map, train_loss=train_loss, val_loss=val_loss, train_acc=train_acc, val_acc=val_acc, checkpoint_dir=os.path.join(CHECKPOINT_DIR, model.__class__.__name__), filename=\"best.pth\")\n",
    "        print(f\"Best model updated at Epoch {epoch+1} with val_loss={val_loss:.4f}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"No improvement for {EARLY_STOPPING_PATIENCE} epochs. Early stopping.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d9535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80051d",
   "metadata": {},
   "source": [
    "# Inferencing\n",
    "Models are inferenced on whole test set quantitatively and qualitatively on single images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162b803-7135-4f48-a28c-579a2ac63769",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28d293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ResNet\n",
    "checkpoint = torch.load('model/ResNet50Model/best.pth')\n",
    "model = ResNet50Model(num_classes=len(checkpoint[\"label_map\"]), pretrained=False)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c21bc-f2c2-4635-beae-2b938ffe4d94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.model import ViTModel\n",
    "\n",
    "# Vision Transformer\n",
    "checkpoint = torch.load('model/ViTModel/best.pth')\n",
    "model = ViTModel(num_classes=len(checkpoint[\"label_map\"]), pretrained=False)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5acb6c-e340-4ca1-89b6-abafdef478f8",
   "metadata": {},
   "source": [
    "## Inference on whole test set\n",
    "- Creates a csv with the image filenames, the ground-truth labels, and the predicted labels\n",
    "- Used for further analyzation to detect weaknesses, strengths, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a521c82e-b7a3-4652-87d3-7b9c3e5a4440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import inference\n",
    "import pandas as pd\n",
    "\n",
    "test_images, test_preds, test_labels = inference(model, test_loader, dataset.label_map, DEVICE)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"Image\": test_images,\n",
    "    \"TrueLabel\": test_labels,\n",
    "    \"PredictedLabel\": test_preds\n",
    "})\n",
    "\n",
    "results_df.to_csv(\"model/test_inference_results.csv\", index=False)\n",
    "print(f\"Saved inference results for {len(test_preds)} samples to test_inference_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504a1ea9-8141-4829-a54f-99a9107b5f75",
   "metadata": {},
   "source": [
    "## Inference single image\n",
    "- Inferences single, random images from the test set\n",
    "- Qualitative evaluation over model predictions\n",
    "- Grad-CAM heatmaps to visualize parts of image the prediction is based on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a355784f-d3cf-4369-a81e-5b8fb9a0517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "def reshape_transform(tensor, height=14, width=14):\n",
    "    result = tensor[:, 1:, :].reshape(tensor.size(0), height, width, tensor.size(2))\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "target_layers = [model.vit.blocks[-1].norm1]\n",
    "\n",
    "idx = random.randint(0, len(test_set) - 1)\n",
    "top_k = 5\n",
    "\n",
    "model.eval()\n",
    "image, label, image_path = dataset[idx]\n",
    "image_tensor = image.unsqueeze(0).to(DEVICE) \n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(image_tensor)\n",
    "    probabilities = torch.softmax(outputs, dim=1).cpu().numpy()[0]\n",
    "\n",
    "topk_indices = probabilities.argsort()[-top_k:][::-1]\n",
    "topk_probs = probabilities[topk_indices]\n",
    "\n",
    "idx_to_name = {v: k for k, v in dataset.label_map.items()}\n",
    "topk_countries = [idx_to_name[i] for i in topk_indices]\n",
    "\n",
    "gt_country = idx_to_name[label]\n",
    "\n",
    "# Grad-CAM\n",
    "cam = GradCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform)\n",
    "targets = [ClassifierOutputTarget(topk_indices[0])]\n",
    "grayscale_cam = cam(input_tensor=image_tensor, targets=targets)[0]\n",
    "\n",
    "input_image = image.permute(1, 2, 0).cpu().numpy()\n",
    "input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "\n",
    "cam_image = show_cam_on_image(input_image, grayscale_cam, use_rgb=True)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(input_image)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title(f\"Ground Truth: {gt_country}\")\n",
    "\n",
    "axs[1].imshow(cam_image)\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title(f\"Grad-CAM: {topk_countries[0]} ({topk_probs[0]*100:.2f}%)\")\n",
    "\n",
    "print(\"Predicted probabilities:\")\n",
    "for country, prob in zip(topk_countries, topk_probs):\n",
    "    print(f\"{country}: {prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9051f2d4-e03b-4661-a216-4e3e0ec52de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
